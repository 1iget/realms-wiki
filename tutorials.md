## [How to Archive The Way I Do](http://gazlenethearchivist.tumblr.com/post/145834158431/how-to-archive-the-way-i-do)

Hi, so first of all, sorry for no recent updates. I’m writing these tutorials on how I archive digital treasures because I’m going on a few-week vacation soon.  

With me, I love the past, and thus I most often archive 1990s/2000s digital material, especially websites. Another reason is because said sites either go offline permanently or have the design changed to something more modern. Either way, for me, it’s upsetting, seeing as I love the Geocities-like websites.

And yes, in case you’re wondering, I also often archive sites that deliberately look as if they’re from 1990s/2000s, so the tutorials might oftentimes be relevant to archiving old websites.

## Table of Contents

1.  “Index of” Searching  

2.  Google Search Tips and Tricks
3.  Wpull/Grab-site Usage
4.  Tubeup, Automatic Video Uploader
5.  File Sharing
6.  Bookmarklets
7.  Search Engine & Wayback Scraping  

8.  Conclusion

## 1\. “Index of“Searching

First, I’ll cover “Index of” directories. I’ll just drop some links here for you lot because it’s very easy to search for index of directories:

*   [https://www.reddit.com/r/opendirectories](http://t.umblr.com/redirect?z=https%3A%2F%2Fwww.reddit.com%2Fr%2Fopendirectories&t=M2NkMTI3ZTIzNWU5ZTIxZGMxMTdiNzIzOTA3ZmVhNzZmMTc1NGQ1Nyx1ZmQ2U05EWg%3D%3D) - Redditors contribute directories on this sub-reddit.  

*   [http://opendir.berkery.nl/](http://t.umblr.com/redirect?z=http%3A%2F%2Fopendir.berkery.nl%2F&t=ZWRjNmNlZTQ2N2Q0OTJmY2I0NDlkMTUyMjg1MDk5ZGE4OGViMzQ1MSx1ZmQ2U05EWg%3D%3D) - Type a subject (preferably in quotes for exact search match), and Google will search every Index of directory containing said keyword.   

*   [http://palined.com/search/](http://t.umblr.com/redirect?z=http%3A%2F%2Fpalined.com%2Fsearch%2F&t=M2E3Y2JlMTU4MTAyNzNlMjNiOGY4YjJiYTA1OTEwODVlNDg0YmQwNSx1ZmQ2U05EWg%3D%3D) - Same as opendir.berkery.nl.  

*   [http://filechef.com/](http://t.umblr.com/redirect?z=http%3A%2F%2Ffilechef.com%2F&t=ZGIxMDcxYzRlYmQ5ZTljZDA1ZTA4ZjMyZWU4MTgwNWVmNTk2MjRkZSx1ZmQ2U05EWg%3D%3D) - More specific than palined or opendir–you can search by music, software, games, etc.

FTP sites are also excellent for “index of” files. The sites I use are [FileMare](http://t.umblr.com/redirect?z=http%3A%2F%2Ffilemare.com%2F&t=Y2M3NTQ1NTg3MTkxNjkyN2ExYmY2YjA5NTY2ZDU4ODY2NDU5ZWFhMix1ZmQ2U05EWg%3D%3D), [Filewatcher](http://t.umblr.com/redirect?z=http%3A%2F%2Ffilewatcher.com%2F&t=OTM5Yzc5YTI3OGYxMzc0ODAyNWE4ZjI5OTg1YWU0ZDQ4ZDlmMjcxYSx1ZmQ2U05EWg%3D%3D), [Mmnt](http://t.umblr.com/redirect?z=http%3A%2F%2Fmmnt.ru%2Fint%2F&t=YjliZDZiMTkzMjUzNTU0MmUwYTg0NWFmOWYyNDBkN2IwMWEwMDRkMCx1ZmQ2U05EWg%3D%3D) (sometimes), and [Mmnt’s open directory](http://t.umblr.com/redirect?z=http%3A%2F%2Fwww.mmnt.net%2F&t=OWM5MjQzODE5NTA5YjliNTBmZjY4ZmMwMmQ1OWVlZmYzZmM3ZmQ3ZSx1ZmQ2U05EWg%3D%3D). See Section 5 on how to upload an Index of site/directory–it’s very similar to what you should do here.

## 2\. Google Search Tips and Tricks

Second, Google operators. Just to save me typing, [here’s a Google Search operator guide](http://t.umblr.com/redirect?z=http%3A%2F%2Fwww.googleguide.com%2Fadvanced_operators_reference.html&t=MzM0OTU4NWE1YmI2NGRkZjc5MjRhOThlNTNiZTAwYmNhNTYwNjY5OCx1ZmQ2U05EWg%3D%3D) ([archive.is copy](http://t.umblr.com/redirect?z=https%3A%2F%2Farchive.is%2FhVKVd&t=ODhhMjJkNDYxYjk3YmFjMjU4NWI1NzNmY2ExM2UyYjJkOWUzM2M0Zix1ZmQ2U05EWg%3D%3D)). You can even combine operators, like intitle: and inurl: I mostly use these:

*   **Quotes (“”):** for a phrase or two or more words. Google will do an exact search for the words you typed in the quotes, like “flying saucers”.  

*   **intitle:puppies:** searches for results containing entered keyword in a site’s title (you can see the title in your browser’s tab).  

*   **inurl:kitties:** searches for an entered keyword in a url. I typed KEYWORD for an example.  

*   **filetype:swf:** Search for a certain filetype in Google. This can be very useful in finding 1990s/2000s websites, e.g., typing filetype:htm will return .htm webpages only–said extension is rarely used anymore. Other good extensions to use are rtf, doc, docx, and html (which I don’t search as much as I should).  

*   **site:.co.uk:** Use this for top-level domain search (e.g., .com, .ca, .mx, etc.). Type **site:example.com** if you want to search by site rather than domain. 

## 3\. Wpull/Grab-site Usage

Third thing I should mention is the most essential part of my archiving journey: wpull/grab-site usage. These are the programs I use to create WARCs (Wayback Machine-readable archives of sites).

I use the former (Wpull) for web hosts, or sites with tons of orphaned pages (i.e., main page redirects, too many unlinked pages). To do this, I scrape search engine results and Wayback Machine results. More info later. I also use wpull to scrape links from a web page. I’ve already wrote a tutorial, which you may read [here](http://t.umblr.com/redirect?z=https%3A%2F%2Ftrello.com%2Fc%2FLGN7B3kL%2F10-download-link-list-with-wpull&t=M2Q3Zjk5NjA1NmE3NDZiZDkxYzEyYWVhNjJlZDk4YWFiYWM1MWE4ZCx1ZmQ2U05EWg%3D%3D).

I use [grab-site](http://t.umblr.com/redirect?z=https%3A%2F%2Fgithub.com%2Fludios%2Fgrab-site&t=OTk4OTNmYzRhYjJhODVlZGQ4YjIzOTdlNDVlZDBiOWQ3ZDExMDI4MSx1ZmQ2U05EWg%3D%3D) to download websites. Simply type “grab-site [http://example.com/”](http://t.umblr.com/redirect?z=http%3A%2F%2Fexample.com%2F%25E2%2580%259D&t=MGM2NTI5MmIxMzc0OGNkOTcyN2E1MGExN2I2M2FkZmMwODY4Y2JjYyx1ZmQ2U05EWg%3D%3D) on the terminal (or, if a website doesn’t have 403/404 results when trying to access directories, list the urls on the terminal, e.g. “grab-site [http://example.com/](http://t.umblr.com/redirect?z=http%3A%2F%2Fexample.com%2F&t=NjdiYTQzYWRjODBiYzgzM2FjZDRmMGFiMzgwMGMyNmQyMDA4NjI1MCx1ZmQ2U05EWg%3D%3D) [http://example.com/1/,](http://t.umblr.com/redirect?z=http%3A%2F%2Fexample.com%2F1%2F%2C&t=YTljYTM4NmIzNGFjN2Q5MTllYmU0NzBhODcxNzM2NDQwMjJjYmMwYSx1ZmQ2U05EWg%3D%3D) etc…” If it’s too much, just use [GoogleScraper](http://t.umblr.com/redirect?z=https%3A%2F%2Fgithub.com%2FNikolaiT%2FGoogleScraper&t=NzMyOWFhODY2ZDNlMTBhMWJhMjkxYWRkNzIyMGViMzRiNGVlZjdmNix1ZmQ2U05EWg%3D%3D).

Seeing as I’ve been lent two VPSs (virtual private servers), I mainly use them for grab-site/wpull. This is very handy if you don’t want to leave your computer running for sites that take a few weeks, a month, or longer to complete.  

After the site finishes downloading, use jjjake’s [internetarchive](http://t.umblr.com/redirect?z=https%3A%2F%2Fgithub.com%2Fjjjake%2Finternetarchive&t=OTcyM2RjZDk5MDRmZWE0MDMwZWQxNTU1ODhhZjJlOWMyZjVlZjNmOSx1ZmQ2U05EWg%3D%3D) script to easily upload files/folders to the Internet Archive from the Terminal, ssh, PuTTy, whichever you use (or go on the web and do [http://archive.org/upload/,](http://t.umblr.com/redirect?z=http%3A%2F%2Farchive.org%2Fupload%2F%2C&t=ZGQ0ZmQ0YWE1OGM1NDE0MTM3NzczNjkzODE4ZGRhZjE0MzRlOTRmYix1ZmQ2U05EWg%3D%3D) but that takes more time and doesn’t work with VPSs and Cygwin–the link works with Windows, obviously). Just don’t push too many requests or you’ll get 503 Slow Down errors for a few hours. This is the format I use for uploading:

> ia upload URL-IDENTIFIER FILE/FOLDERNAME \
> 
> –metadata=“title:ITEMTITLEGOESHERE” \
> 
> –metadata=“subject:hashtag;like;stuff;goes;here;separated;by;semicolons” \
> 
> –metadata=“collection:opensource_media”

Here’s an example:

> ia upload warc-example_com example.com-2016[…] \
> 
> –metadata=“title:WARC: example.com” \
> 
> –metadata=“subject:warcarchives;example;boring”

I haven’t entered a collection because I’m fine with WARCs being in community texts. You may want to add –metadata=“mediatype:web” to give the WARCs a web mediatype.

Here’s an important thing for me to mention: if the upload doesn’t work, then check the identifier in your browser to see whether somebody’s already uploaded an existing item there or not. If so, change the identifier to something not taken.  You are also able to upload additional items to the items you’ve uploaded, and you should only use this format: “ia upload same-url-identifier file-folder-goeshere”. 

 Additionally, if you haven’t downloaded your S3 keys, refer to the internetarchive GitHub page on how to get them.  

Here are a list of available collections, with self-explanatory titles, for any user to upload to (there are more collections, except only restricted to certain users or more commonly, staff members). Adding the appropriate collection is recommended just to organize your upload properly:

*   opensource_media: Open Source Media
*   open_source_software: Community Software
*   opensource_audio: Community Audio
*   opensource_movies: Community Video

Just wait for it to finish, and then you’re done.

## 4\. Tubeup, Automatic Video Uploader

Fourth thing I should mention is tubeup, a tool that auto-uploads youtube-dl-compatible videos to the Internet Archive (e.g. Dailymotion, YouTube, Vimeo, many sites). See here for the script: [https://github.com/bibanon/tubeup](http://t.umblr.com/redirect?z=https%3A%2F%2Fgithub.com%2Fbibanon%2Ftubeup&t=NWQyZTM5MWUxNzkzMWNkMDM4YTQ4Y2FiOGIxNjhlZjkxMmM5MjdjOSx1ZmQ2U05EWg%3D%3D) . This tool is meant for individual videos only, unless a channel only has 2-3 videos. You can also type more than one url in the Terminal, e.g. “python3 tubeup.py url1 url2″.

If a channel has like 50 videos or so, use this method to download the channel rather than pushing too many requests to the Internet Archive:  

_youtube-dl -q –download-archive ~/.ytdlarchive –retries 100 –no-overwrites –call-home –continue –write-info-json –write-description –write-thumbnail –write-annotations –all-subs –sub-format srt –convert-subs srt –write-sub –add-metadata -f bestvideo+bestaudio/best –merge-output-format ‘mkv’ –ignore-errors CHANNELGOESHERE_

Thank you to vxbinaca for creating this tutorial (though, I added the –ignore-errors parameter at the end so youtube-dl doesn’t terminate on YouTube videos for copyright claim reasons, etc.)

After downloading the entire channel, type “tar cvf filename.tar FOLDERNAME/” on the terminal to compress all of the videos in a tar file (to avoid pushing too many requests or internetarchive not working on videos with certain characters in them, like #’s–I had to learn this myself).

You can also add “–remove-files” after “FOLDERNAME”  in the “tar cvf” command  to delete the files once they’re compressed in the TAR file if you’re low on disk space. After compressing, upload to the Internet Archive using this command:  

> ia upload (yt|vimeo|etc)-urlidentifieronvideosite channelname.tar \
> 
> –metadata=“title:VIDEO HOST Account: CHANNEL NAME (TAR)” \
> 
> –metadata=“subject:youtube;topic;channel;whatever;etc” \
> 
> –metadata=“collection:opensource_media”
> 
> Or you can type –metadata:“collection:opensource_movies”  (either applies–TARs just don’t have video playback, so that’s why I added opensource_media for an option.)

Let me provide you guys an example:

> ia upload yt-muffinsandcupcakes64 muffinsandcupcakes64.tar \
> 
> –metadata=“title:YouTube Account:  muffinsandcupcakes64 (TAR)″ \
> 
> –metadata=“subject:youtube;muffins;cupcakes;funny;stupid″ \
> 
> –metadata=“collection:opensource_media”

**Note:** Okay, so since Google+, YouTube doesn’t always use the _youtube.com/user/username/_ format in their urls–sometimes, it’s _youtube.com/channel/bladhdhnehrandomjunk/_. For me, if the channel doesn’t have a /user/username/ format, I’d add the _bladhdhnehrandomjunk_ in the IA identifier. E.g., yt-_bladhdhnehrandomjunk, not yt-muffinsandcupcakes64_.

## 5\. File Sharing

Another thing I like to archive are file sharing folders and files. I’ve discovered [DDLSearch](http://t.umblr.com/redirect?z=http%3A%2F%2Fddlsearch.free.fr%2F&t=ODQxYTk2MDYwNDVmMTM3YzI4ZjRlYjZkYjliM2QxZmY1MjUxODFmZSx1ZmQ2U05EWg%3D%3D), and it’s the best file search out there. You select a search engine, file host, and topic, and then it searches for you. There are so many search engines to use though: FileCatch, MegaSearch.us,

If you find a file sharing link you want to archive, I recommend using [jDownloader](http://t.umblr.com/redirect?z=http%3A%2F%2Fjdownloader.org&t=MTI3MDlmNDgwM2JmZjk5ODAxZmIyZjQ2YjViNWY4OTYwMmVkMzNjNSx1ZmQ2U05EWg%3D%3D) over anything else. Just uncheck the clipboard icon before using, or else it will copy every link you access!

Okay, so copy+paste folders and files (you can do numerous at once) into jDownloader and click “Start All Downloads”. Just note that you’ll probably have to type a lot of captchas with many file hosts, notably Mediafire.

Once the download(s) finish, upload to the Internet Archive either manually or with the internetarchive script. I’ll provide an upload example (similar to the youtube-dl example listed above):

> ia upload mediafire-26PDFs 26PDFs/ \
> 
> –metadata=“title:Mediafire: 26 PDFs” \
> 
> –metadata=“subject:mediafire;pdfs”
> 
> –metadata=“collection:opensource_media” (a best choice honestly changes–anything can be a part of file sharing).

Also, for torrents, you use the same method. I use [torrentz.eu](http://t.umblr.com/redirect?z=http%3A%2F%2Ftorrentz.eu%2F&t=OWJmMjhlYjkxMWVmOTYzMjUzMGY0M2RhYTkzYWU5YmMzMzFlZjgyMCx1ZmQ2U05EWg%3D%3D) for a torrent search engine.

## 6\. Bookmarklets 

I love using these bookmarklets for searching for images/webpages on the Wayback Machine and saving web pages to archive.is. The location and appropriate title for each are provided below:

*   javascript:location.href='http://web.archive.org/web/*/’+location.href - Search on archive.org  

*   javascript:location.href='http://web.archive.org/save/’+location.href - Save to archive.org  

*   javascript:void(open('https://archive.is/?run=1&url=’+encodeURIComponent(document.location))) - archive.is  

## 7\. Search Engine & Wayback Scraping

I’ve mentioned earlier that I use wpull to download search engine and Wayback Machine results, and now here’s my tutorial on how to do so.

First, I’ll be teaching to scrape the search engine results, and then the Wayback Machine results. I pretty much only use GoogleScraper for web hosts and occasionally some sites with orphaned links–I haven’t found a reason to use keywords or file extension-specific searches yet.

I recommend using Cloud9, seeing as I use it on there, especially if you’re using Windows. I think I’ve been told it’s not safe for usage on VPSs, but it’s more than easy to use on Cloud9, even with the Free option.

Anyway, here’s the format for downloading indexed results from Google, Yandex, Bing, and DuckDuckGo altogether (Baidu is compatible but it acts weird with these commands). Also, I owe a big thanks to r3c0d3x for creating a url.js to filter the links from the rest of the junk.  

_GoogleScraper -m http -p NUMOFPAGES -n 50 -s “bing,google,duckduckgo,yandex” -q “site:example.com” -o out.json && node urls.js && rm out.json_

Okay, so before I go any farther, I’m going to mention Notepad++,  a Windows-specific program, because it is part of a Windows-specific part of this tutorial written below. (personally, I use said OS most often, but that’s beside the point). There is a segment of this tutorial written for Linux and Windows each. 

**Linux:**

After GoogleScraper is done scraping results, copy and paste all of the results from the urls.txt file and into a new document of your text editor (I’ve only tried nano, but vim and gedit look like good options themselves–using nano saves you time so you don’t have to manually open a GUI program). You’re done with the search engine results.

Now, to scrape the Wayback results, download this url with wget or wpull (replace urlgoeshere with the appropriate site name–the asterisk at the end is to list all of the results of a url): [http://web.archive.org/cdx/search/cdx?url=urlgoeshere*](http://t.umblr.com/redirect?z=http%3A%2F%2Fweb.archive.org%2Fcdx%2Fsearch%2Fcdx%3Furl%3Durlgoeshere%2A&t=NjkwNTM2ZmJjMjFlMjUxNjZmZWE4NjczZWE5MjBmYzBlNDc5MTBlOSx1ZmQ2U05EWg%3D%3D). Also take note that sub-domains aren’t a part of these results, unlike the search engines. Here’s an example of a download:  

wget [http://web.archive.org/cdx/search/cdx?url=http://example.com*](http://t.umblr.com/redirect?z=http%3A%2F%2Fweb.archive.org%2Fcdx%2Fsearch%2Fcdx%3Furl%3Durlgoeshere%2A&t=NjkwNTM2ZmJjMjFlMjUxNjZmZWE4NjczZWE5MjBmYzBlNDc5MTBlOSx1ZmQ2U05EWg%3D%3D) 

Rename the ridiculously long download name by typing mv cdx[…] links.txt (the links.txt name can be changed to a name of your choice).  Now filter the links using this command:

_ls TEXTFILE.EXT | grep -v 'output.txt’ | xargs grep -iIohE 'https?://[^ ]+’ >> OUTPUT.EXT_

I haven’t tried writing to the same file, but I’d suggest doing a different file just to be on the safe side.

Just so wpull doesn’t download duplicates, filter the links. You can either use [Alphabetizer](http://t.umblr.com/redirect?z=http%3A%2F%2Fwww.textfixer.com%2Ftools%2Falphabetize-text-words.php&t=MTk5NGY5OGFlODhjMzlkNTNjZTMwYWVjMDUwZWU1M2ZmMTRlM2NlYix1ZmQ2U05EWg%3D%3D) and check _Line Break Separator_ & uncheck _Remove Punctuation_, (websites tend to die–I prefer using this site but it may die any time). Here’s a Linux terminal method in case Textfixer dies:

_sort OUTPUT.EXT | uniq -u >> OUTPUT2.EXT_

After doing that, merge the Wayback & search engine results into a single text file and download using this command (I recommend keeping the log, WARC, and db names all the same–this command is inspired from the one on [Github’s Wpull page](http://t.umblr.com/redirect?z=https%3A%2F%2Fgithub.com%2Fchfoo%2Fwpull&t=ZTE3NDY3ZDFmY2JhNmY1YzE5OTc5YTM3N2MyNGEwZGYzNTQxMWVkOSx1ZmQ2U05EWg%3D%3D)):

_wpull -i OUTPUT2.EXT –page-requisites -r –no-robots –no-check-certificate –tries 3 –timeout 60 –delete-after –warc-file WARCFILENAME –warc-max-size=4294967296 –database _WARCFILENAME_ .db –output-file _WARCFILENAME_ .log –user-agent “InconspiuousWebBrowser/1.0”_

Now, wait for wpull to finish (it could very well take days). Once it’s done, upload to the Internet Archive using the ia upload format I used in section 3 above. Then you’re done, congrats!

**Windows:**

For Windows, copy and paste all of the results from the urls.txt file and into a new Notepad++ document. Then hover to _TextFX–>TextFX Tools–>Sort lines case-insensitive_ to alphabetize and delete duplicates of the links.

After doing that, it’s time to scrape the Wayback Machine results. Now open either cygwin or puTTy (if you have a VPS for the latter), and like before, wget the cdx listing, rename the downloaded link, and filter and de-dupe the links with either Alphabetizer or from a terminal.

Now merge the Wayback links with the search engine links, and use wpull to download them. Since wpull is incompatible with cygwin, you should either use a VPS, Cloud9, or a Linux machine on VirtualBox.

Anyway, wait for it to finish uploading, and then you’re done.

## Conclusion

Wow, wasn’t that such a long read? I sure think so. I spent hours typing up all of this, and I hope it’s helped you guys learn how to become a better archivist. Even I can improve, I admit that. There’s a saying that goes “You never stop improving.”

Bye bye, and good luck with digital and web archiving!
